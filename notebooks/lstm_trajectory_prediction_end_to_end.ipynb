{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End LSTM Trajectory Prediction with OpenSky Data\n",
        "\n",
        "This notebook demonstrates a complete workflow for aircraft trajectory prediction using the LSTMMultiHorizon model.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook covers:\n",
        "1. **Data Loading**: OpenSky historical data or synthetic data generation\n",
        "2. **Feature Preprocessing**: Process sequences with 8 core features (velocity, track, time-of-day)\n",
        "3. **Model Training**: Train a multi-layer LSTM model for multi-horizon trajectory prediction\n",
        "4. **Evaluation**: Assess performance using ADE/FDE metrics and visualizations\n",
        "\n",
        "### Features Used (8 total)\n",
        "- **vx, vy, vz**: Velocity components in East, North, Up (m/s)\n",
        "- **speed**: Horizontal speed (m/s)\n",
        "- **cos_track, sin_track**: Track angle encoding\n",
        "- **sin_tod, cos_tod**: Time of day encoding\n",
        "\n",
        "### Model Output\n",
        "- Predicts future positions as offsets (\u0394e, \u0394n, \u0394u) in ENU coordinates\n",
        "- Multi-horizon: predicts K future timesteps from H historical timesteps\n",
        "\n",
        "## OpenSky Historical Data Integration\n",
        "\n",
        "This notebook is now integrated with the **OpenSky historical data ingestion pipeline**:\n",
        "\n",
        "### Using OpenSky Data\n",
        "\n",
        "**Step 1**: Download historical flight data from OpenSky Network\n",
        "- Visit: https://opensky-network.org/datasets/states/\n",
        "- Download state vectors in Parquet format (e.g., `opensky_states_2024-01.parquet`)\n",
        "\n",
        "**Step 2**: Process the data using the ingestion script\n",
        "```bash\n",
        "python ../opensky_historical_ingestion.py --input /path/to/opensky_states.parquet --output data/raw/opensky_data.csv\n",
        "```\n",
        "\n",
        "**Step 3**: The script outputs a CSV with columns compatible with our pipeline:\n",
        "- `flight_id`: Aircraft identifier (ICAO24 code)\n",
        "- `timestamp`: Unix timestamp (seconds)\n",
        "- `lat`, `lon`: Geographic coordinates (degrees)\n",
        "- `alt`: Altitude (meters, filtered to `alt > 0`)\n",
        "\n",
        "**Step 4**: Load and use the data in this notebook (see Data Loading section below)\n",
        "\n",
        "### Alternative: Synthetic Data\n",
        "\n",
        "For quick experimentation without external dependencies, you can generate synthetic data using the built-in data generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Environment\n",
        "\n",
        "First, we'll set up the environment by installing dependencies and importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# Uncomment the following line if running in Colab or if packages are not installed\n",
        "# !pip install -q torch numpy pandas matplotlib seaborn tqdm scikit-learn pyproj pymap3d pyyaml requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Deep learning imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path if not already there\n",
        "import os\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "print(\"\u2713 Directories created\")\n",
        "print(\"\u2713 Python path configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading: OpenSky Historical Data or Synthetic\n",
        "\n",
        "You can choose between two data sources:\n",
        "\n",
        "### Option A: OpenSky Historical Data (Recommended for Production)\n",
        "\n",
        "Use real aircraft trajectory data from the OpenSky Network. This provides production-quality training data with real flight patterns.\n",
        "\n",
        "**Prerequisites**:\n",
        "1. Download OpenSky parquet file from https://opensky-network.org/datasets/states/\n",
        "2. Process it using: `python ../opensky_historical_ingestion.py --input <parquet_file> --output data/raw/opensky_data.csv`\n",
        "\n",
        "The processed CSV will have the required format:\n",
        "- **flight_id**: Aircraft ICAO24 identifier\n",
        "- **timestamp**: Unix timestamp (seconds)\n",
        "- **lat, lon**: Geographic coordinates (degrees)\n",
        "- **alt**: Altitude in meters (filtered to positive values)\n",
        "\n",
        "### Option B: Synthetic Data (Quick Start)\n",
        "\n",
        "Generate synthetic flight trajectories for immediate experimentation. This is useful for:\n",
        "- Testing the pipeline without external data\n",
        "- Quick prototyping and development\n",
        "- Reproducible experiments\n",
        "\n",
        "**Data will be generated around Hong Kong International Airport (VHHH)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# OPTION A: Load OpenSky Historical Data (uncomment to use)\n",
        "# ================================================================================\n",
        "\n",
        "# import os\n",
        "# \n",
        "# # Path to your processed OpenSky CSV file\n",
        "# # This file should be created by running:\n",
        "# # python ../opensky_historical_ingestion.py --input <parquet_file> --output data/raw/opensky_data.csv\n",
        "# OPENSKY_CSV = 'data/raw/opensky_data.csv'\n",
        "# \n",
        "# if os.path.exists(OPENSKY_CSV):\n",
        "#     print(f\"Loading OpenSky historical data from: {OPENSKY_CSV}\")\n",
        "#     df_raw = pd.read_csv(OPENSKY_CSV)\n",
        "#     \n",
        "#     print(f\"Raw data shape: {df_raw.shape}\")\n",
        "#     print(f\"Columns: {list(df_raw.columns)}\")\n",
        "#     print(f\"Number of unique flights: {df_raw['flight_id'].nunique()}\")\n",
        "#     print(f\"Time span: {pd.to_datetime(df_raw['timestamp'].min(), unit='s')} to {pd.to_datetime(df_raw['timestamp'].max(), unit='s')}\")\n",
        "#     print(f\"Altitude range: {df_raw['alt'].min():.0f}m to {df_raw['alt'].max():.0f}m\")\n",
        "#     \n",
        "#     # Filter to ensure positive altitudes (pipeline requirement)\n",
        "#     df_raw = df_raw[df_raw['alt'] > 0]\n",
        "#     print(f\"\\nAfter altitude filtering (alt > 0): {len(df_raw)} rows\")\n",
        "#     \n",
        "#     # Display first few rows\n",
        "#     print(\"\\nFirst few rows:\")\n",
        "#     display(df_raw.head())\n",
        "#     \n",
        "#     print(\"\\n\u2713 OpenSky historical data loaded successfully\")\n",
        "# else:\n",
        "#     print(f\"Error: OpenSky CSV file not found at {OPENSKY_CSV}\")\n",
        "#     print(\"\\nPlease follow these steps:\")\n",
        "#     print(\"1. Download OpenSky parquet file from: https://opensky-network.org/datasets/states/\")\n",
        "#     print(\"2. Run: python ../opensky_historical_ingestion.py --input <parquet_file> --output data/raw/opensky_data.csv\")\n",
        "#     print(\"3. Re-run this cell\")\n",
        "#     raise FileNotFoundError(f\"OpenSky CSV not found: {OPENSKY_CSV}\")\n",
        "\n",
        "# ================================================================================\n",
        "# OPTION B: Generate Synthetic Data (default)\n",
        "# ================================================================================\n",
        "\n",
        "# Option B: Generate synthetic data (recommended for demo)\n",
        "# This creates realistic synthetic flights around Hong Kong (VHHH)\n",
        "\n",
        "print(\"Generating synthetic Hong Kong airspace data...\")\n",
        "!python ../src/simulate_data.py \\\n",
        "    --num_flights 600 \\\n",
        "    --min_len 160 \\\n",
        "    --max_len 260 \\\n",
        "    --out_csv data/raw/hkg_data.csv\n",
        "\n",
        "print(\"\\n\u2713 Synthetic data generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: The following cells work with BOTH OpenSky historical data and synthetic data\n",
        "# because both produce CSV files with the same format:\n",
        "# flight_id, timestamp, lat, lon, alt\n",
        "\n",
        "# If using OpenSky data (Option A above), change the file path:\n",
        "# df_raw = pd.read_csv('data/raw/opensky_data.csv')  # For OpenSky data\n",
        "\n",
        "# If using synthetic data (Option B - default):\n",
        "df_raw = pd.read_csv('data/raw/hkg_data.csv')  # For synthetic data\n",
        "\n",
        "# The rest of the pipeline works identically for both data sources\n",
        "print(f\"Raw data shape: {df_raw.shape}\")\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nNumber of unique flights: {df_raw['flight_id'].nunique()}\")\n",
        "print(f\"Time span: {df_raw['timestamp'].min()} to {df_raw['timestamp'].max()}\")\n",
        "print(f\"Altitude range: {df_raw['alt'].min():.0f}m to {df_raw['alt'].max():.0f}m\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few rows:\")\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and inspect the raw data\n",
        "df_raw = pd.read_csv('data/raw/hkg_data.csv')\n",
        "\n",
        "print(f\"Raw data shape: {df_raw.shape}\")\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nNumber of unique flights: {df_raw['flight_id'].nunique()}\")\n",
        "print(f\"Time span: {df_raw['timestamp'].min()} to {df_raw['timestamp'].max()}\")\n",
        "print(f\"Altitude range: {df_raw['alt'].min():.0f}m to {df_raw['alt'].max():.0f}m\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few rows:\")\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Preprocessing\n",
        "\n",
        "Now we'll preprocess the data to create input sequences for the LSTM model:\n",
        "\n",
        "### Steps:\n",
        "1. **Region Filtering**: Keep only flights within Hong Kong airspace\n",
        "2. **Temporal Resampling**: Standardize time intervals (e.g., 30 seconds)\n",
        "3. **Coordinate Transformation**: Convert lat/lon/alt to local ENU (East-North-Up) coordinates\n",
        "4. **Feature Engineering**: Compute velocity, speed, track angle, and time-of-day features\n",
        "5. **Sequence Windowing**: Create (history, future) pairs for training\n",
        "\n",
        "### Parameters:\n",
        "- **input_len** (H): Number of historical timesteps (e.g., 40)\n",
        "- **pred_len** (K): Number of future timesteps to predict (e.g., 20)\n",
        "- **resample_sec**: Time resolution in seconds (e.g., 30s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the preprocessing pipeline\n",
        "# This creates sequences suitable for LSTM training\n",
        "\n",
        "INPUT_LEN = 40  # History window (H)\n",
        "PRED_LEN = 20   # Prediction horizon (K)\n",
        "RESAMPLE_SEC = 30  # Time resolution\n",
        "REGION = \"113.5,115.5,21.5,23.0\"  # Hong Kong region (lon_min,lon_max,lat_min,lat_max)\n",
        "\n",
        "print(\"Preprocessing data into sequences...\")\n",
        "!python ../src/prepare_sequences.py \\\n",
        "    --input_csv data/raw/hkg_data.csv \\\n",
        "    --output_npz data/processed/hkg_seq.npz \\\n",
        "    --input_len {INPUT_LEN} \\\n",
        "    --pred_len {PRED_LEN} \\\n",
        "    --resample_sec {RESAMPLE_SEC} \\\n",
        "    --region \"{REGION}\" \\\n",
        "    --min_points 120\n",
        "\n",
        "print(\"\\n\u2713 Preprocessing complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and inspect the preprocessed data\n",
        "data = np.load('data/processed/hkg_seq.npz', allow_pickle=True)\n",
        "\n",
        "X = data['X']  # Input sequences: [N, H, F] where F=8 features\n",
        "Y = data['Y']  # Target sequences: [N, K, 3] where 3=(\u0394e, \u0394n, \u0394u)\n",
        "flights = data['flights']  # Flight IDs for each sequence\n",
        "\n",
        "print(f\"Input sequences (X): {X.shape}\")\n",
        "print(f\"  - N: {X.shape[0]} sequences\")\n",
        "print(f\"  - H: {X.shape[1]} timesteps (history)\")\n",
        "print(f\"  - F: {X.shape[2]} features\")\n",
        "print(f\"\\nTarget sequences (Y): {Y.shape}\")\n",
        "print(f\"  - N: {Y.shape[0]} sequences\")\n",
        "print(f\"  - K: {Y.shape[1]} timesteps (future)\")\n",
        "print(f\"  - 3: (east, north, up) offsets in meters\")\n",
        "print(f\"\\nUnique flights: {len(np.unique(flights))}\")\n",
        "\n",
        "# Feature names for reference\n",
        "feature_names = ['vx', 'vy', 'vz', 'speed', 'cos_track', 'sin_track', 'sin_tod', 'cos_tod']\n",
        "print(f\"\\nFeatures: {feature_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample trajectories in ENU coordinates\n",
        "# Plot a few examples to understand the data\n",
        "\n",
        "n_samples = 4\n",
        "idx = np.random.choice(len(Y), size=n_samples, replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_samples, figsize=(16, 4))\n",
        "for i, ax in enumerate(axes):\n",
        "    y_sample = Y[idx[i]]  # [K, 3]\n",
        "    # Plot future trajectory offsets\n",
        "    ax.plot(y_sample[:, 0], y_sample[:, 1], 'o-', linewidth=2, markersize=4)\n",
        "    ax.plot([0], [0], 'r*', markersize=15, label='Start (last history point)')\n",
        "    ax.axhline(0, color='gray', linewidth=0.5, linestyle='--')\n",
        "    ax.axvline(0, color='gray', linewidth=0.5, linestyle='--')\n",
        "    ax.set_xlabel('East offset (m)')\n",
        "    ax.set_ylabel('North offset (m)')\n",
        "    ax.set_title(f'Sample {idx[i]}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    if i == 0:\n",
        "        ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/notebook_run/sample_trajectories.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Sample trajectories saved to outputs/notebook_run/sample_trajectories.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Architecture: LSTMMultiHorizon\n",
        "\n",
        "Our model uses a multi-layer LSTM network for sequence-to-sequence prediction:\n",
        "\n",
        "### Architecture:\n",
        "```\n",
        "Input: [Batch, H, F] where F=8 features\n",
        "  \u2193\n",
        "LSTM Layers (with dropout)\n",
        "  \u2193\n",
        "Take last hidden state: [Batch, hidden_size]\n",
        "  \u2193\n",
        "FC Layer \u2192 ReLU \u2192 Dropout \u2192 FC Layer\n",
        "  \u2193\n",
        "Output: [Batch, K, 3] where 3=(\u0394e, \u0394n, \u0394u)\n",
        "```\n",
        "\n",
        "### Hyperparameters:\n",
        "- **hidden_size**: LSTM hidden dimension (e.g., 128)\n",
        "- **num_layers**: Number of stacked LSTM layers (e.g., 2)\n",
        "- **dropout**: Dropout rate for regularization (e.g., 0.2)\n",
        "\n",
        "Let's import and inspect the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import model from src\n",
        "try:\n",
        "    from model import LSTMMultiHorizon\n",
        "except ImportError:\n",
        "    # If direct import fails, try with explicit src prefix\n",
        "    import sys\n",
        "    import os\n",
        "    src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "    if src_path not in sys.path:\n",
        "        sys.path.insert(0, src_path)\n",
        "    from model import LSTMMultiHorizon\n",
        "\n",
        "# Create model instance\n",
        "INPUT_SIZE = 8  # Number of features\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "model = LSTMMultiHorizon(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    pred_len=PRED_LEN\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model with dummy input to verify output shape\n",
        "dummy_input = torch.randn(4, INPUT_LEN, INPUT_SIZE)  # [Batch=4, H=40, F=8]\n",
        "dummy_output = model(dummy_input)\n",
        "\n",
        "print(f\"Input shape:  {dummy_input.shape}  # [Batch, History, Features]\")\n",
        "print(f\"Output shape: {dummy_output.shape}  # [Batch, Prediction, 3]\")\n",
        "print(\"\\n\u2713 Model forward pass successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "We'll train the model using:\n",
        "- **Loss Function**: Smooth L1 Loss (Huber loss) - robust to outliers\n",
        "- **Optimizer**: AdamW with weight decay for regularization\n",
        "- **Validation Metrics**: ADE (Average Displacement Error) and FDE (Final Displacement Error)\n",
        "- **Early Stopping**: Stop when validation ADE stops improving\n",
        "\n",
        "### Training Process:\n",
        "1. Split data into train/val/test sets (by flight ID to avoid leakage)\n",
        "2. Train for multiple epochs\n",
        "3. Validate after each epoch\n",
        "4. Save best model based on validation ADE\n",
        "5. Plot training curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import training utilities\n",
        "from dataset import TrajSeqDataset\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Create datasets (split by flights to avoid leakage)\n",
        "ds_train = TrajSeqDataset('data/processed/hkg_seq.npz', split='train', seed=SEED)\n",
        "ds_val = TrajSeqDataset('data/processed/hkg_seq.npz', split='val', seed=SEED)\n",
        "ds_test = TrajSeqDataset('data/processed/hkg_seq.npz', split='test', seed=SEED)\n",
        "\n",
        "print(f\"Train set: {len(ds_train)} sequences\")\n",
        "print(f\"Val set:   {len(ds_val)} sequences\")\n",
        "print(f\"Test set:  {len(ds_test)} sequences\")\n",
        "print(f\"\\nTotal:     {len(ds_train) + len(ds_val) + len(ds_test)} sequences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ds_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    ds_val,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Batches per epoch: {len(train_loader)} (train), {len(val_loader)} (val)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = LSTMMultiHorizon(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    pred_len=PRED_LEN\n",
        ").to(device)\n",
        "\n",
        "# Optimizer and loss\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 0.0\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "criterion = nn.SmoothL1Loss()\n",
        "\n",
        "print(\"\u2713 Model, optimizer, and criterion initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define ADE and FDE computation\n",
        "def compute_ade_fde(pred, target):\n",
        "    \"\"\"\n",
        "    Compute Average Displacement Error and Final Displacement Error.\n",
        "    \n",
        "    Args:\n",
        "        pred: [B, K, 3] predicted offsets in meters\n",
        "        target: [B, K, 3] ground truth offsets in meters\n",
        "    \n",
        "    Returns:\n",
        "        ade: Average displacement error (mean over all timesteps)\n",
        "        fde: Final displacement error (at last timestep)\n",
        "    \"\"\"\n",
        "    diff = pred - target\n",
        "    dist = torch.linalg.norm(diff, dim=-1)  # [B, K]\n",
        "    ade = dist.mean().item()\n",
        "    fde = dist[:, -1].mean().item()\n",
        "    return ade, fde\n",
        "\n",
        "print(\"\u2713 Metrics defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "EPOCHS = 20\n",
        "GRAD_CLIP = 1.0\n",
        "PATIENCE_LIMIT = 5\n",
        "\n",
        "best_val_ade = float('inf')\n",
        "patience = 0\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'val_ade': [],\n",
        "    'val_fde': []\n",
        "}\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ========== Training Phase ==========\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Train]\"):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        pred = model(xb)\n",
        "        loss = criterion(pred, yb)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # ========== Validation Phase ==========\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_ade, all_fde = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Val]  \"):\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            \n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            val_loss += loss.item()\n",
        "            \n",
        "            # Compute metrics\n",
        "            ade, fde = compute_ade_fde(pred, yb)\n",
        "            all_ade.append(ade)\n",
        "            all_fde.append(fde)\n",
        "    \n",
        "    val_loss /= len(val_loader)\n",
        "    val_ade = float(np.mean(all_ade))\n",
        "    val_fde = float(np.mean(all_fde))\n",
        "    \n",
        "    # Record history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_ade'].append(val_ade)\n",
        "    history['val_fde'].append(val_fde)\n",
        "    \n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch {epoch:2d}/{EPOCHS}: \"\n",
        "          f\"train_loss={train_loss:.4f}, \"\n",
        "          f\"val_loss={val_loss:.4f}, \"\n",
        "          f\"val_ADE={val_ade:.2f}m, \"\n",
        "          f\"val_FDE={val_fde:.2f}m\")\n",
        "    \n",
        "    # ========== Early Stopping & Model Saving ==========\n",
        "    if val_ade < best_val_ade - 1e-3:\n",
        "        best_val_ade = val_ade\n",
        "        patience = 0\n",
        "        \n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), 'outputs/notebook_run/best/model.pt')\n",
        "        with open('outputs/notebook_run/best/meta.txt', 'w') as f:\n",
        "            f.write(f\"epoch={epoch}\\nval_ADE={val_ade}\\nval_FDE={val_fde}\\n\")\n",
        "        print(f\"  \u2192 New best model saved (val_ADE: {val_ade:.2f}m)\")\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= PATIENCE_LIMIT:\n",
        "            print(f\"\\nEarly stopping triggered (patience={PATIENCE_LIMIT})\")\n",
        "            break\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "print(f\"Best validation ADE: {best_val_ade:.2f}m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Smooth L1 Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Metrics curves\n",
        "axes[1].plot(history['val_ade'], label='Val ADE (m)', linewidth=2, marker='o')\n",
        "axes[1].plot(history['val_fde'], label='Val FDE (m)', linewidth=2, marker='s')\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Error (meters)', fontsize=12)\n",
        "axes[1].set_title('Validation Metrics', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/notebook_run/training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Training curves saved to outputs/notebook_run/training_curves.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n",
        "\n",
        "Now let's evaluate the trained model on the test set and compare it with a constant velocity baseline.\n",
        "\n",
        "### Evaluation Metrics:\n",
        "- **ADE (Average Displacement Error)**: Mean Euclidean distance between predicted and true positions across all timesteps\n",
        "- **FDE (Final Displacement Error)**: Euclidean distance at the final timestep only\n",
        "\n",
        "### Baseline:\n",
        "We'll compare against a simple constant velocity model that extrapolates using the last 5 velocity measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('outputs/notebook_run/best/model.pt', map_location=device))\n",
        "model.eval()\n",
        "print(\"\u2713 Best model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test data loader\n",
        "test_loader = DataLoader(\n",
        "    ds_test,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "all_pred, all_true = [], []\n",
        "\n",
        "print(\"Evaluating on test set...\")\n",
        "with torch.no_grad():\n",
        "    for xb, yb in tqdm(test_loader, desc=\"Test\"):\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).cpu().numpy()\n",
        "        all_pred.append(pred)\n",
        "        all_true.append(yb.numpy())\n",
        "\n",
        "pred_test = np.concatenate(all_pred, axis=0)\n",
        "true_test = np.concatenate(all_true, axis=0)\n",
        "\n",
        "print(f\"\\nTest predictions shape: {pred_test.shape}\")\n",
        "print(f\"Test ground truth shape: {true_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute test metrics\n",
        "def compute_ade_fde_np(pred, true):\n",
        "    \"\"\"Compute ADE/FDE using NumPy.\"\"\"\n",
        "    diff = pred - true\n",
        "    dist = np.linalg.norm(diff, axis=-1)  # [N, K]\n",
        "    ade = float(dist.mean())\n",
        "    fde = float(dist[:, -1].mean())\n",
        "    return ade, fde\n",
        "\n",
        "test_ade, test_fde = compute_ade_fde_np(pred_test, true_test)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL PERFORMANCE ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ADE (Average Displacement Error): {test_ade:.2f} meters\")\n",
        "print(f\"FDE (Final Displacement Error):   {test_fde:.2f} meters\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constant velocity baseline\n",
        "def constant_velocity_baseline(X_hist, pred_len, dt=30):\n",
        "    \"\"\"\n",
        "    Simple baseline: extrapolate using mean velocity from last 5 timesteps.\n",
        "    \n",
        "    Args:\n",
        "        X_hist: [H, F] with vx, vy, vz as features 0, 1, 2\n",
        "        pred_len: K, number of future steps\n",
        "        dt: time resolution in seconds\n",
        "    \n",
        "    Returns:\n",
        "        offsets: [K, 3] predicted offsets\n",
        "    \"\"\"\n",
        "    vx = X_hist[-5:, 0].mean()\n",
        "    vy = X_hist[-5:, 1].mean()\n",
        "    vz = X_hist[-5:, 2].mean()\n",
        "    \n",
        "    steps = np.arange(1, pred_len + 1, dtype=np.float32).reshape(-1, 1)\n",
        "    offsets = np.hstack([vx * steps * dt, vy * steps * dt, vz * steps * dt])\n",
        "    return offsets\n",
        "\n",
        "# Compute baseline on a subset (for efficiency)\n",
        "print(\"Computing constant velocity baseline...\")\n",
        "n_baseline = min(2000, len(ds_test))\n",
        "idx_baseline = np.random.choice(len(ds_test), size=n_baseline, replace=False)\n",
        "\n",
        "Xs_baseline = ds_test.X[idx_baseline]\n",
        "Ys_baseline = ds_test.Y[idx_baseline]\n",
        "\n",
        "baseline_pred = np.stack(\n",
        "    [constant_velocity_baseline(Xs_baseline[i], PRED_LEN, RESAMPLE_SEC) \n",
        "     for i in tqdm(range(n_baseline), desc=\"Baseline\")],\n",
        "    axis=0\n",
        ")\n",
        "\n",
        "baseline_ade, baseline_fde = compute_ade_fde_np(baseline_pred, Ys_baseline)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASELINE PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ADE: {baseline_ade:.2f} meters\")\n",
        "print(f\"FDE: {baseline_fde:.2f} meters\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: MODEL vs BASELINE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<20} {'Model':<15} {'Baseline':<15} {'Improvement':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'ADE (meters)':<20} {test_ade:<15.2f} {baseline_ade:<15.2f} {(1 - test_ade/baseline_ade)*100:>13.1f}%\")\n",
        "print(f\"{'FDE (meters)':<20} {test_fde:<15.2f} {baseline_fde:<15.2f} {(1 - test_fde/baseline_fde)*100:>13.1f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save results\n",
        "with open('outputs/notebook_run/test_metrics.txt', 'w') as f:\n",
        "    f.write(f\"Model ADE: {test_ade:.3f}\\n\")\n",
        "    f.write(f\"Model FDE: {test_fde:.3f}\\n\")\n",
        "    f.write(f\"Baseline ADE: {baseline_ade:.3f}\\n\")\n",
        "    f.write(f\"Baseline FDE: {baseline_fde:.3f}\\n\")\n",
        "\n",
        "print(\"\\n\u2713 Test metrics saved to outputs/notebook_run/test_metrics.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization: Predictions vs Ground Truth\n",
        "\n",
        "Let's visualize some sample predictions to qualitatively assess the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot sample trajectory predictions\n",
        "n_samples = 6\n",
        "sample_idx = np.random.choice(len(true_test), size=n_samples, replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, idx in enumerate(sample_idx):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Get true and predicted trajectories\n",
        "    true_traj = true_test[idx]  # [K, 3]\n",
        "    pred_traj = pred_test[idx]  # [K, 3]\n",
        "    \n",
        "    # Plot in 2D (East-North plane)\n",
        "    ax.plot(true_traj[:, 0], true_traj[:, 1], 'o-', \n",
        "            label='Ground Truth', linewidth=2.5, markersize=6, alpha=0.8)\n",
        "    ax.plot(pred_traj[:, 0], pred_traj[:, 1], 's-', \n",
        "            label='LSTM Prediction', linewidth=2.5, markersize=6, alpha=0.8)\n",
        "    \n",
        "    # Mark start point\n",
        "    ax.plot([0], [0], 'r*', markersize=20, label='Start', zorder=10)\n",
        "    \n",
        "    # Reference lines\n",
        "    ax.axhline(0, color='gray', linewidth=0.8, linestyle='--', alpha=0.5)\n",
        "    ax.axvline(0, color='gray', linewidth=0.8, linestyle='--', alpha=0.5)\n",
        "    \n",
        "    # Compute error for this sample\n",
        "    sample_error = np.linalg.norm(true_traj - pred_traj, axis=-1).mean()\n",
        "    \n",
        "    ax.set_xlabel('East Offset (m)', fontsize=11)\n",
        "    ax.set_ylabel('North Offset (m)', fontsize=11)\n",
        "    ax.set_title(f'Sample {idx} (ADE: {sample_error:.1f}m)', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(fontsize=9, loc='best')\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/notebook_run/predictions_vs_groundtruth.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Prediction visualizations saved to outputs/notebook_run/predictions_vs_groundtruth.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error distribution histogram\n",
        "# Compute per-timestep errors across all test samples\n",
        "errors = np.linalg.norm(pred_test - true_test, axis=-1)  # [N, K]\n",
        "errors_flat = errors.flatten()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of all errors\n",
        "axes[0].hist(errors_flat, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(test_ade, color='red', linewidth=2, linestyle='--', label=f'Mean ADE: {test_ade:.2f}m')\n",
        "axes[0].set_xlabel('Displacement Error (m)', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Distribution of Displacement Errors', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Error vs timestep\n",
        "mean_errors_per_step = errors.mean(axis=0)  # [K]\n",
        "std_errors_per_step = errors.std(axis=0)    # [K]\n",
        "timesteps = np.arange(1, PRED_LEN + 1)\n",
        "\n",
        "axes[1].plot(timesteps, mean_errors_per_step, 'o-', linewidth=2, markersize=6, label='Mean Error')\n",
        "axes[1].fill_between(timesteps, \n",
        "                      mean_errors_per_step - std_errors_per_step,\n",
        "                      mean_errors_per_step + std_errors_per_step,\n",
        "                      alpha=0.3, label='\u00b11 Std Dev')\n",
        "axes[1].set_xlabel('Prediction Timestep', fontsize=12)\n",
        "axes[1].set_ylabel('Displacement Error (m)', fontsize=12)\n",
        "axes[1].set_title('Error Growth Over Prediction Horizon', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/notebook_run/error_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Error analysis saved to outputs/notebook_run/error_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 3D Trajectory Visualization\n",
        "\n",
        "Let's also visualize trajectories in 3D to see vertical movement (altitude changes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3D trajectory visualization\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "n_samples_3d = 3\n",
        "sample_idx_3d = np.random.choice(len(true_test), size=n_samples_3d, replace=False)\n",
        "\n",
        "fig = plt.figure(figsize=(16, 5))\n",
        "\n",
        "for i, idx in enumerate(sample_idx_3d):\n",
        "    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
        "    \n",
        "    true_traj = true_test[idx]\n",
        "    pred_traj = pred_test[idx]\n",
        "    \n",
        "    # Plot trajectories\n",
        "    ax.plot(true_traj[:, 0], true_traj[:, 1], true_traj[:, 2], \n",
        "            'o-', label='Ground Truth', linewidth=2, markersize=5, alpha=0.8)\n",
        "    ax.plot(pred_traj[:, 0], pred_traj[:, 1], pred_traj[:, 2], \n",
        "            's-', label='Prediction', linewidth=2, markersize=5, alpha=0.8)\n",
        "    \n",
        "    # Start point\n",
        "    ax.scatter([0], [0], [0], c='red', s=200, marker='*', label='Start', zorder=10)\n",
        "    \n",
        "    ax.set_xlabel('East (m)', fontsize=10)\n",
        "    ax.set_ylabel('North (m)', fontsize=10)\n",
        "    ax.set_zlabel('Up (m)', fontsize=10)\n",
        "    ax.set_title(f'3D Trajectory - Sample {idx}', fontsize=12, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/notebook_run/3d_trajectories.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 3D trajectory visualizations saved to outputs/notebook_run/3d_trajectories.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Saved Outputs\n",
        "\n",
        "### What We Accomplished:\n",
        "1. \u2713 Fetched/generated aircraft trajectory data\n",
        "2. \u2713 Preprocessed data into sequences with 8 features\n",
        "3. \u2713 Trained a multi-layer LSTM model\n",
        "4. \u2713 Evaluated with ADE/FDE metrics\n",
        "5. \u2713 Visualized predictions vs ground truth\n",
        "6. \u2713 Saved all results and model checkpoints\n",
        "\n",
        "### Saved Files:\n",
        "- `data/raw/hkg_data.csv` - Raw trajectory data\n",
        "- `data/processed/hkg_seq.npz` - Preprocessed sequences\n",
        "- `outputs/notebook_run/best/model.pt` - Best model weights\n",
        "- `outputs/notebook_run/training_curves.png` - Training/validation curves\n",
        "- `outputs/notebook_run/test_metrics.txt` - Final test metrics\n",
        "- `outputs/notebook_run/predictions_vs_groundtruth.png` - Prediction visualizations\n",
        "- `outputs/notebook_run/error_analysis.png` - Error distribution and growth\n",
        "- `outputs/notebook_run/3d_trajectories.png` - 3D trajectory plots\n",
        "\n",
        "### Next Steps:\n",
        "- **Use Real Data**: Replace synthetic data with OpenSky API data\n",
        "- **Add Weather Features**: Incorporate wind (u/v) from GFS/ERA5\n",
        "- **Hyperparameter Tuning**: Experiment with different architectures\n",
        "- **Ensemble Methods**: Combine multiple models\n",
        "- **Deploy**: Create inference API for real-time predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary printout\n",
        "print(\"=\"*70)\n",
        "print(\" \"*15 + \"END-TO-END LSTM TRAJECTORY PREDICTION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n\ud83d\udcca DATASET STATISTICS:\")\n",
        "print(f\"  Total sequences:      {len(ds_train) + len(ds_val) + len(ds_test):,}\")\n",
        "print(f\"  Train sequences:      {len(ds_train):,}\")\n",
        "print(f\"  Validation sequences: {len(ds_val):,}\")\n",
        "print(f\"  Test sequences:       {len(ds_test):,}\")\n",
        "print(f\"  History length (H):   {INPUT_LEN} timesteps\")\n",
        "print(f\"  Prediction length (K): {PRED_LEN} timesteps\")\n",
        "print(f\"  Features:             {INPUT_SIZE}\")\n",
        "\n",
        "print(f\"\\n\ud83e\udde0 MODEL ARCHITECTURE:\")\n",
        "print(f\"  Type:                 LSTMMultiHorizon\")\n",
        "print(f\"  Hidden size:          {HIDDEN_SIZE}\")\n",
        "print(f\"  LSTM layers:          {NUM_LAYERS}\")\n",
        "print(f\"  Dropout:              {DROPOUT}\")\n",
        "print(f\"  Total parameters:     {trainable_params:,}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 TRAINING:\")\n",
        "print(f\"  Epochs completed:     {len(history['train_loss'])}\")\n",
        "print(f\"  Batch size:           {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate:        {LEARNING_RATE}\")\n",
        "print(f\"  Best val ADE:         {best_val_ade:.2f}m\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf TEST RESULTS:\")\n",
        "print(f\"  Model ADE:            {test_ade:.2f} meters\")\n",
        "print(f\"  Model FDE:            {test_fde:.2f} meters\")\n",
        "print(f\"  Baseline ADE:         {baseline_ade:.2f} meters\")\n",
        "print(f\"  Baseline FDE:         {baseline_fde:.2f} meters\")\n",
        "print(f\"  Improvement (ADE):    {(1 - test_ade/baseline_ade)*100:.1f}%\")\n",
        "print(f\"  Improvement (FDE):    {(1 - test_fde/baseline_fde)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcbe SAVED OUTPUTS:\")\n",
        "saved_files = [\n",
        "    'data/raw/hkg_data.csv',\n",
        "    'data/processed/hkg_seq.npz',\n",
        "    'outputs/notebook_run/best/model.pt',\n",
        "    'outputs/notebook_run/training_curves.png',\n",
        "    'outputs/notebook_run/test_metrics.txt',\n",
        "    'outputs/notebook_run/predictions_vs_groundtruth.png',\n",
        "    'outputs/notebook_run/error_analysis.png',\n",
        "    'outputs/notebook_run/3d_trajectories.png'\n",
        "]\n",
        "for f in saved_files:\n",
        "    print(f\"  \u2713 {f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \"*20 + \"\ud83c\udf89 ALL STEPS COMPLETED SUCCESSFULLY! \ud83c\udf89\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}